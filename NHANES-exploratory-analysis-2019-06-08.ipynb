{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import nhanes as nhanes\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'CDC/NHANES/'\n",
    "DATASET = 'cancer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "The code below loads each dataset: dataset_features, dataset_targets\n",
    "\n",
    "Here, all datasets are defined explicitly (see nhanes.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = nhanes.Dataset(DATA_PATH)\n",
    "ds.load_arthritis()\n",
    "n_fe = ds.features.shape[1]\n",
    "n_classes = 2\n",
    "\n",
    "dataset_features = ds.features\n",
    "dataset_targets = ds.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(dataset_targets.shape[0])\n",
    "dataset_features = dataset_features[perm]\n",
    "dataset_targets = dataset_targets[perm]\n",
    "\n",
    "def get_batch(n_size, phase):\n",
    "    # select indices\n",
    "    n_samples = dataset_features.shape[0]\n",
    "    n_classes = int(dataset_targets.max() + 1)\n",
    "    if phase == 'test':\n",
    "        inds_sel = np.arange(0, int(n_samples*0.15), 1)\n",
    "    elif phase == 'validation':\n",
    "        n_samples = dataset_features.shape[0]\n",
    "        inds_sel = np.arange(int(n_samples*0.15), int(n_samples*0.30), 1)\n",
    "    elif phase == 'train':\n",
    "        n_samples = dataset_features.shape[0]\n",
    "        inds_sel = np.arange(int(n_samples*0.30), n_samples, 1)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    inds_sel = np.random.permutation(inds_sel)\n",
    "    batch_inds = []\n",
    "    for cl in range(n_classes):\n",
    "        inds_cl = inds_sel[dataset_targets[inds_sel] == cl]\n",
    "        batch_inds.extend(inds_cl[:n_size//n_classes])\n",
    "    batch_inds = np.random.permutation(batch_inds)\n",
    "    \n",
    "    return dataset_features[batch_inds], dataset_targets[batch_inds]\n",
    "    \n",
    "features_trn, targets_trn = get_batch(n_size=8000, phase='train')\n",
    "features_tst, targets_tst = get_batch(n_size=2000, phase='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(features_trn, targets_trn)\n",
    "preds_tst = clf.predict(features_tst)\n",
    "accu = np.mean(preds_tst==targets_tst)\n",
    "print('accu_tst_RFC', accu)\n",
    "\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(features_trn, targets_trn)\n",
    "preds_tst = clf.predict(features_tst)\n",
    "accu = np.mean(preds_tst==targets_tst)\n",
    "print('accu_tst_SVC', accu)\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "clf.fit(features_trn, targets_trn)\n",
    "preds_tst = clf.predict(features_tst)\n",
    "accu = np.mean(preds_tst==targets_tst)\n",
    "print('accu_tst_LR', accu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Performance Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_perf(df,y):\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    rf_score = cross_val_score(rf,df,y,cv=5)\n",
    "    \n",
    "    svm = SVC(gamma='auto')\n",
    "    svm_score = cross_val_score(svm,df,y,cv=5)\n",
    "    \n",
    "    lr = LogisticRegression(solver='lbfgs',max_iter=200)\n",
    "    lr_score = cross_val_score(lr,df,y,cv=5)\n",
    "    \n",
    "    return [rf_score.mean(),svm_score.mean(),lr_score.mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection/Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove correlated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(features_trn)\n",
    "X_test = pd.DataFrame(features_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_perf(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corr = X_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.heatmap(X_corr, cmap='coolwarm',xticklabels=False,yticklabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = []\n",
    "for i in range(len(X_corr)):\n",
    "    for j in range(len(X_corr)):\n",
    "        if (i != j) and (X_corr.iloc[i,j]>0.7):\n",
    "            if ([i,j] not in to_drop) and ([j,i] not in to_drop):\n",
    "                to_drop.append([i,j])\n",
    "\n",
    "idxs = []\n",
    "for i in to_drop:\n",
    "    if (i[0] not in idxs) and (i[1] not in idxs):\n",
    "        idxs.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(idxs,axis=1,inplace=True)\n",
    "X_test.drop(idxs,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_perf(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corr_post = X_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.heatmap(X_corr, cmap='coolwarm',xticklabels=False,yticklabels=False)\n",
    "plt.title('Pre-Filter')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.heatmap(X_corr_post, cmap='coolwarm',xticklabels=False,yticklabels=False)\n",
    "plt.title('Post-Filter')\n",
    "\n",
    "plt.savefig('corrfilter.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual Information filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.Series(targets_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = mutual_info_classif(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(mi,bins=50)\n",
    "plt.xlabel('Mutual Information')\n",
    "plt.ylabel('# of Features')\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.boxplot(x=mi)\n",
    "plt.xlabel('Mutual Information')\n",
    "\n",
    "plt.savefig('mi.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_cut = np.percentile(mi,25)\n",
    "to_keep = (mi > mi_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.loc[:,to_keep]\n",
    "X_test = X_test.loc[:,to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_perf(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_idxs = []\n",
    "cont_idxs = []\n",
    "for i in X_train.columns:\n",
    "    if X_train.loc[:,i].nunique() <= 2:\n",
    "        bin_idxs.append(i)\n",
    "    else:\n",
    "        cont_idxs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bin = X_train.loc[:,bin_idxs]\n",
    "X_train_cont = X_train.loc[:,cont_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_var = X_train_bin.var() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_cut = np.percentile(bin_var,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bin = X_train_bin.loc[:,(bin_var > bin_cut)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_var = X_train_cont.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.hist(bin_var,bins=50)\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('# of Features')\n",
    "plt.title('Categorical')\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "sns.boxplot(x=bin_var)\n",
    "plt.xlabel('Variance')\n",
    "plt.title('Categorical')\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.hist(cont_var,bins=10)\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('# of Features')\n",
    "plt.title('Continuous')\n",
    "\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "sns.boxplot(x=cont_var)\n",
    "plt.xlabel('Variance')\n",
    "plt.title('Continuous')\n",
    "\n",
    "\n",
    "plt.savefig('varfil.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cut = np.percentile(cont_var,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cont = X_train_cont.loc[:,(cont_var > cont_cut)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_bin,X_train_cont],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.loc[:,X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_perf(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.logspace(-4,4,20)\n",
    "scores = []\n",
    "for i in C:\n",
    "    print(i)\n",
    "    lrfs = LogisticRegression(solver='liblinear',penalty='l1',C=i)\n",
    "    cvs = cross_val_score(lrfs,X_train,y_train, cv=5)\n",
    "    scores.append(cvs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(np.log(C),scores,marker='o',color='blue')\n",
    "\n",
    "plt.xlabel('Regularization Coefficient (Log Scale)')\n",
    "plt.ylabel('C-V Accuracy')\n",
    "plt.title('Lasso Optimization')\n",
    "plt.savefig('lasso.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfs = LogisticRegression(penalty='l1',C=C[7])\n",
    "lrfs.fit(X_train,targets_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.loc[:,(lrfs.coef_ > 0)[0]]\n",
    "X_test = X_test.loc[:,(lrfs.coef_ > 0)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "X_train_poly = pd.DataFrame(X_train_poly)\n",
    "X_test_poly = pd.DataFrame(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_perf(X_train_poly,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_poly = mutual_info_classif(X_train_poly,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(mi_poly,bins=50)\n",
    "plt.xlabel('Mutual Information of Polynomial Features')\n",
    "plt.ylabel('# of Features')\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.boxplot(x=mi_poly)\n",
    "plt.xlabel('Mutual Information of Polynomial Features')\n",
    "\n",
    "#plt.savefig('mi.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_cut = np.percentile(mi_poly,90)\n",
    "to_keep = (mi_poly > mi_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly = X_train_poly.loc[:,to_keep]\n",
    "X_test_poly = X_test_poly.loc[:,to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = np.percentile(X_train_poly.var(),90)\n",
    "X_train_poly = X_train_poly.loc[:,(X_train_poly.var() > cut)]\n",
    "X_test_poly = X_test_poly.loc[:,(X_train_poly.var() > cut)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_perf(X_train_poly,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs_pca = []\n",
    "n = range(5,51,5)\n",
    "for i in n:\n",
    "    print(i)\n",
    "    pca_ = PCA(n_components=i)\n",
    "    X_pca = pca_.fit_transform(X_train)\n",
    "    gbdt_pca = GradientBoostingClassifier(n_estimators=50,min_samples_split=0.5)\n",
    "    cvs = cross_val_score(gbdt_pca,X_pca,y_train, cv=5)\n",
    "    cvs_pca.append(cvs.mean())\n",
    "\n",
    "print(cvs_pca)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = []\n",
    "for i in range(1,21):\n",
    "    dt = DecisionTreeClassifier(max_depth=i)\n",
    "    cvs = cross_val_score(dt,X_train,y_train, cv=5)\n",
    "    cv.append(cvs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(list(range(1,21)),cv,marker='o',color='blue')\n",
    "\n",
    "plt.xlabel('DTree Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Decision Tree Optimization')\n",
    "plt.savefig('Dtree_opt.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [0.1,1,10]\n",
    "kernel = ['linear','rbf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cv = 0\n",
    "combo = None\n",
    "for i in C:\n",
    "    for j in kernel:\n",
    "        print([i,j])\n",
    "        svc = SVC(C=i,kernel=j)\n",
    "        cvs = cross_val_score(svc,X_train,y_train, cv=5)\n",
    "        curr_cv = cvs.mean()\n",
    "        print(curr_cv)\n",
    "        if curr_cv > max_cv:\n",
    "            max_cv = curr_cv\n",
    "            combo = [i,j]\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [50,100,150]\n",
    "n_samp = [0.1,0.3,0.5,0.7]\n",
    "n_feat = [0.1,0.3,0.5,0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cv = 0\n",
    "combo = None\n",
    "for i in n_est:\n",
    "    for j in n_samp:\n",
    "        for k in n_feat:\n",
    "            print([i,j,k])\n",
    "            rf = RandomForestClassifier(n_estimators=i,min_samples_split=j,max_features=k)\n",
    "            cvs = cross_val_score(rf,X_train,y_train, cv=5)\n",
    "            curr_cv = cvs.mean()\n",
    "            print(curr_cv)\n",
    "            if curr_cv > max_cv:\n",
    "                max_cv = curr_cv\n",
    "                combo = [i,j,k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [60]\n",
    "n_samp = np.array((range(5,16,1)))/100\n",
    "n_feat = np.array((range(5,16,1)))/100\n",
    "\n",
    "rf_60 = pd.DataFrame(data=np.zeros((11,11)),index=n_samp,columns=n_feat)\n",
    "for i in n_est:\n",
    "    for j in n_samp:\n",
    "        for k in n_feat:\n",
    "            rf = RandomForestClassifier(n_estimators=i,min_samples_split=j,max_features=k)\n",
    "            cvs = cross_val_score(rf,X_train,y_train, cv=5)\n",
    "            curr_cv = cvs.mean()\n",
    "            rf_60.loc[j,k] = curr_cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.heatmap(rf_40,cbar=False,cmap='coolwarm')\n",
    "plt.xlabel('Feat %')\n",
    "plt.ylabel('Samp %')\n",
    "plt.title('N_est = 40')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.heatmap(rf_50,cbar=False,yticklabels=False,cmap='coolwarm')\n",
    "plt.xlabel('Feat %')\n",
    "plt.title('N_est = 50')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(rf_60,yticklabels=False,cmap='coolwarm')\n",
    "plt.xlabel('Feat %')\n",
    "plt.title('N_est = 60')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rfopt.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [150]\n",
    "n_samp = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "cv150 = []\n",
    "for i in n_est:\n",
    "    for j in n_samp:\n",
    "        print([i,j])\n",
    "        gbdt = GradientBoostingClassifier(n_estimators=i,min_samples_split=j)\n",
    "        cvs = cross_val_score(gbdt,X_train,y_train, cv=5)\n",
    "        curr_cv = cvs.mean()\n",
    "        cv150.append(curr_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [70]\n",
    "n_samp = np.array(range(5,16,1))/100\n",
    "\n",
    "cv70 = []\n",
    "for i in n_est:\n",
    "    for j in n_samp:\n",
    "        print([i,j])\n",
    "        gbdt = GradientBoostingClassifier(n_estimators=i,min_samples_split=j)\n",
    "        cvs = cross_val_score(gbdt,X_train,y_train, cv=5)\n",
    "        curr_cv = cvs.mean()\n",
    "        cv70.append(curr_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(n_samp,cv30,marker='o',color='red',label='30 ests')\n",
    "plt.plot(n_samp,cv40,marker='o',color='orange',label='40 ests')\n",
    "plt.plot(n_samp,cv50,marker='o',color='yellow',label='50 ests')\n",
    "plt.plot(n_samp,cv60,marker='o',color='green',label='60 ests')\n",
    "plt.plot(n_samp,cv70,marker='o',color='blue',label='70 ests')\n",
    "\n",
    "plt.xlabel('Sample %')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Gradient-Boosted Decision Tree Optimization')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('gbdtopt.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt = GradientBoostingClassifier(n_estimators=30,min_samples_split=0.2)\n",
    "gbdt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.Series(data = gbdt.feature_importances_, index=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_fi = fi.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=top_fi.values,y=top_fi.index,orient='h',order=top_fi.index)\n",
    "\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature Index')\n",
    "plt.title('GBDT Feature Importances')\n",
    "\n",
    "plt.savefig('topfi.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(df, dataset_targets, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te.columns = [str(i) for i in X_te.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te = X_te.loc[:,X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = gbdt.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_te,final_preds))\n",
    "print('\\n')\n",
    "print(accuracy_score(y_te,final_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.columns = [str(i) for i in X_tr.columns]\n",
    "X_tr = X_tr.loc[:,X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt2 = GradientBoostingClassifier(n_estimators=30,min_samples_split=0.2)\n",
    "gbdt2.fit(X_tr,y_tr)\n",
    "preds2 = gbdt2.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_te,preds2))\n",
    "print('\\n')\n",
    "print(accuracy_score(y_te,preds2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Densely Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "def partition (list_in, n):\n",
    "    random.shuffle(list_in)\n",
    "    return [list_in[i::n] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folds\n",
    "idx_folds = partition(list(X_train.index),5)\n",
    "\n",
    "tr1 = idx_folds[0] + idx_folds[1] + idx_folds[2] + idx_folds[3]\n",
    "tr2 = idx_folds[0] + idx_folds[2] + idx_folds[3] + idx_folds[4]\n",
    "tr3 = idx_folds[0] + idx_folds[1] + idx_folds[3] + idx_folds[4]\n",
    "tr4 = idx_folds[0] + idx_folds[1] + idx_folds[2] + idx_folds[4]\n",
    "tr5 = idx_folds[1] + idx_folds[2] + idx_folds[3] + idx_folds[4]\n",
    "tr = [tr1,tr2,tr3,tr4,tr5]\n",
    "\n",
    "te1 = idx_folds[4]\n",
    "te2 = idx_folds[1]\n",
    "te3 = idx_folds[2]\n",
    "te4 = idx_folds[3]\n",
    "te5 = idx_folds[0]\n",
    "te = [te1,te2,te3,te4,te5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns = [str(i) for i in X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs = []\n",
    "for i in range(len(tr)):\n",
    "    print(i)\n",
    "    #Suppress output except for error messages\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    #feature columns\n",
    "    feature_columns = []\n",
    "    for j in X_train.columns:\n",
    "        feature_columns.append(tf.feature_column.numeric_column(str(j)))\n",
    "\n",
    "    #Model Initialization\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        hidden_units=[100, 50, 25],\n",
    "        optimizer=tf.train.AdamOptimizer(1e-2),\n",
    "        n_classes=2,\n",
    "        dropout=0.2,\n",
    "    )\n",
    "\n",
    "    #Input Function\n",
    "    train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "        x=X_train.iloc[tr[i],:],\n",
    "        y=y_train.iloc[tr[i]],\n",
    "        num_epochs=50,\n",
    "        batch_size=len(tr[i]),\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    #model training\n",
    "    classifier.train(input_fn=train_input_fn, steps=10000)\n",
    "\n",
    "    #eval function\n",
    "    test_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "        x=X_train.iloc[te[i],:],\n",
    "        y=y_train.iloc[te[i]],\n",
    "        num_epochs=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    #predictions\n",
    "    preds = list(classifier.predict(test_input_fn))\n",
    "\n",
    "    pred_class = [p[\"classes\"] for p in preds]\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for k in range(len(pred_class)):\n",
    "        preds.append(int(pred_class[k][0]))\n",
    "    \n",
    "    acc = accuracy_score(y_train.iloc[te[i]],preds)\n",
    "    print(acc)\n",
    "    \n",
    "    cvs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cvs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
